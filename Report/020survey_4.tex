\subsection{Asynchronous Actor-Critic}
Volodymyr Mnih, et. al. 이 제안한 Asynchronous Actor-Critic (A3C)~\cite{A3C}는 게임 플레이를 위한 강화학습에 널리 쓰이는 방법이다.
A3C에서 Asynchronous는 여러 agent가 각자의 environment에서 활동하며 비동기적으로 global network를 업데이트해 나가는 것을 의미한다.
앞서 DQN이나 DDPG에서 학습에 활용되는 샘플 데이터의 상관관계를 줄이기 위해 Replay Memory를 사용하였던 것을 대체하는 것으로 독립된 여러 agent가 서로 다른 environment에서 샘플을 추출하므로 각각의 업데이트에 활용되는 데이터의 상관관계는 낮다.

Actor-Critic은 특정 Policy를 통해 action을 취하는 actor와 그것을 value function을 통해 평가하는 critic으로 이루어진 강화학습의 한 구조이다.
이 구조에서는 Policy loss와 Value loss를 이용해 모델을 학습시키며 각각을 식으로 표현하면 아래와 같다.
\begin{align*}
	L_p &= log(\pi(s)) \times A(s) + \beta \times H(\pi) \\
	L_v &= \sum {(R-V(s))}^2  \\
\end{align*}
Policy loss $L_p$에서 policy $\pi(s)$는 advantage $A(s)$에 의해 평가되며 advantage가 양수이면 action은 1의 방향, 음수이면 0의 방향으로 학습된다.
A3C 논문에서는 entropy loss $H(\pi)$를 추가하여 모델이 보수적으로 early converge되는 것을 막는다.
Value loss $L_v$에서는 reward $R$과 value function $V(s)$의 간극을 줄이고자 한다.
최종적인 loss는 아래와 같이 계산된다.
\begin{align*}
	Loss &= L_p + 0.5 \times L_v \\
\end{align*}

A3C에서 각 agent는 아래와 같은 동작을 비동기적으로 수행하며 global network를 업데이트한다.
\begin{enumerate}
	\item
각각의 thread에서 동작하는 agent는 global network로부터 parameter 를 복사
	\item
각 agent는 서로 다른 environment에서 각자의 exploration policy로 탐색
	\item
각 agent는 각자의 loss를 계산
	\item
각 agent는 각자의 gradient를 계산
	\item
각 agent는 각자의 gradient를 이용하여 global network를 업데이트
\end{enumerate}

A3C는 5종의 아타리 게임에서 DQN에 비해 짧은 학습시간을 소모하여 더 높은 성능을 보여주었다.
또한 단일 머신에서 thread를 늘려가며 agent의 수를 증가하여 학습시킬 수 있으며, 이 때 모델의 성능이 효과적으로 증가한다.
놀랍게도 agent 수에 대해 선형적 이상의 성능향상을 보여주기도 하는데, 이는 agent의 수가 적을 때 편향된 학습을 하게 되는 것을 보완하기 때문이라고 해석된다. 
