%We implemented our method
%and compared it with the older ones.
%The results are very promising.

%Figure \ref{fig:results} shows our results:
%Figure \ref{fig:results}(a) gives a scatter-plot
%of the $N$ sound-clips, where the axis are the two main
%features we propose to use $\ldots$
%Figure \ref{fig:results}(b) shows the wall-clock time
%of our method, versus the size of the database $N$.

%\begin{figure}[htb]
%\begin{center}
%\begin{tabular}{cc}
%     % uncomment the next lines, and give the right ps files
%     \includegraphics[width=0.3\textwidth]{FIG/data.png} &
%     \includegraphics[width=0.3\textwidth]{FIG/plot.png} \\
%     %\psfig{figure=FIG/plot.ps,width=2in} \\
%     % \psfig{figure=FIG/data.ps,width=2in} &
%     % \psfig{figure=FIG/plot.ps,width=2in} \\
%    (a) & (b) 
%\end{tabular}
%\caption{A fictitious dataset (a) and its performance plot (b)}
%\label{fig:results}
%\end{center}
%\end{figure}

%In this section, we explain our experiments performed to evaluate our proposed method to answer following questions:
%\bit
%\item {\bf Q1.Accuracy.} How accurately does our proposed method predicts next choice of a user?
%\item {\bf Q2.Sustainability.} Is it necessary to re-calcluate all of the previous data for updating user/article representation?
%\eit

%We experiment to show the performance of our proposed method, \methodname, comparing with several competitors.
%All of the competitors and our method share the news recommendation system structure which is for preprocessing the data and evaluating the methods.
%We change the article representation and the recurrent function as the methods for making the fair competition environment for all of the competitors and our methods.

We describe the common experiment settings in Section~\ref{sec:exp:settings}.
We introduce the preprocessing procedure of data in Section~\ref{sec:exp:preprocess}.
We mention how to train the model in Section~\ref{sec:exp:training}.
These preprocessing and the traning process are shared to all of competitors and our methods.
We experiment to answer following questions to evaluate our proposed methods.
\begin{itemize}
	\item \textbf{Q1. Performance (Section~\ref{sec:exp:results}).}
		How better the \method than the competitors as news recommendation system?
	\item \textbf{Q2. Cold-start problem (Section~\ref{sec:exp:results}).}
		Does \method relieve the Cold-Start problem to recommend new articles appropriately?
	\item \textbf{Q3. Effects of information from recent articles (Section~\ref{sec:exp:discuss}).}
		Does the information from recent articles increases the performance of recommendation system?
		%The information from the popular articles is directly related to the next article in the session.
		%How about the information from the recent articles?
	\item \textbf{Q4. Effects of attention and cell dropout (Section~\ref{sec:exp:discuss}).}
		How much the attention and cell dropout improve the performance of \methodname?
\end{itemize}

\subsection{Experiment Settings}
We describe common experiment settings to evaluate the competitors and our methods.
\label{sec:exp:settings}
\paragraph{\textbf{Dataset}}
\begin{table}[h]
	\renewcommand{\arraystretch}{1.2}
	\caption{
		Overall dataset statistics after the preprocessing.
		We remove the events with an invalid URLs in the session in the \dataAdressaName.
	}
	\label{tab:dataset}
	\begin{tabular}{m{1.90cm}R{1.43cm}R{1.19cm}R{1.36cm}C{0.94cm}}
\toprule
		\textbf{Dataset} & \textbf{\# Sessions} & \textbf{\# Events} & \textbf{\# Articles} & \textbf{Peroid} \\
\midrule
\dataAdressaOneWeekName\footnotemark[1] & 112,405 & 487,961 & 11,069 & 10 days \\
\dataAdressaTenWeekName\footnotemark[1] & 655,790 & 8,167,390 & 43,460 & 90 days \\
\dataGloboName\footnotemark[2] & 296,332 & 2,994,717 & 46,577 & 16 days \\
\bottomrule
	\end{tabular}
\end{table}
We prepare the two datasets which provide the session-based sequences of the news article selections for users.
The Adresseavisen is the one of the newspaper media owned by the Polaris Media in Norway.
Gulla et al.~\cite{AdressaDataset} kindly provide two versions of the Adressa dataset (\dataAdressaName)\footnote{http://reclab.idi.ntnu.no/dataset}, only the difference between them is the period, one of which is for one week and the other is for ten weeks.
The one week version of the Adressa dataset (\dataAdressaOneWeekName) contains the 2.7 million of events on the Adresseavisen's news portal from 1 to 7 January 2017.
And the ten weeks version (\dataAdressaTenWeekName) includes 20 million of events from 1 January to 31 March 2017.
They do not provide the contents of the news article but the related URL instead.
So we crawl the contents of each article from the URL to generate the representation of the article from the sentences.

The G1.com is the news portal site which is maintained by Grupo Globo in Brazil.
Moreira et al.~\cite{Chameleon} kindly provide the Globo dataset (\dataGloboName)\footnote{https://www.kaggle.com/gspmoreira/news-portal-user-interactions-by-globocom} which contains the user interaction of the G1 news portal from 1 to 16 October 2017.
The Globo dataset includes the 3 million of events separated by the 1.2 million sessions.
Because of the license restrictions, they do not provide the full text of the news article but the 250-dimensional embedding vectors of the article which had been generated by their pretrained module using the contents and the meta-information of the article.

\paragraph{\textbf{Competitor}}
We introduce the competitor methods to be compared to our proposed method.
Most of the competitors (except the \compPopName) are based on the RNN to make a recommendation from the session-based data.
They make the change within the article representation or the recurrent function in the model.
The \compPop is the only method which is not trained from the data.
We generate the candidate articles for each timestamp which is sorted by the descending order of popularity in the preprocessing phase.
We just rank the candidate articles with the popularity in the \compPopName.

The competitor method, \compLSTMName, use the LSTM cell as the recurrent function in the model.
The \compLSTM is fed with the session-based input to extract the user preferences from their behavior.
The LSTM cell is well known to capture both long-term and short-term preferences to extract features of the sequence.
We add the linear layer after the hidden state of the \compLSTM to generate the prediction vector of the next article representation.

The \compGruRec consists of the multi-layered Gated Recurrent Unit (GRU)~\cite{GRU}.
We add the linear layer after the last hidden state of the \compGruRec and do the dropout the node in the linear layer with fixed probability to prevent the overfitting.

The recurrent function of the \compNaver is the LSTM cell which is the same with the \compLSTMName.
The \compNaver uses the category information of the article to relieve the Cold-Start problem.
We concatenate the one-hot encoded category information to the hidden state and give additional score regard on the category of the candidate article.

The \compYahoo uses the multi-layered GRU as the recurrent function.
We prepare the triple of the article, the article with the same category, and with the other category.
We train the Variational Autoencoder (AVE) model with this triple to generate new article representaion which are closer between articles in the same category and farther in the other categories.

%Chung et al.~\cite{GRU} proposed the Gated Recurrent Unit (GRU) as the abbreviated structure of the LSTM cell.
%Hidasi et al.~\cite{GRU4Rec} published their research about using the multi-layered GRU (\compGruRecName) for the recommendation system with the session-based input.
%They show that the multi-layered GRU outperformed the other competitors which are used for the recommendation task.
%
%The Naver is the web portal service in South Korea.
%Park et al.~\cite{Naver2017} published their research about the news recommendation system tested with the news content service on the front page of the Naver.
%In their method (\compNaverName), they aggregate the category information of news article to the model when training.
%Because of the sparsity of the meta-information, they introduced the Convolutional Neural Network (CNN)~\cite{CNNHinton} based method to predict the empty entry for the category in the data.
%They utilize the category information when ranking the candidate articles that they keep the preference about the category in the session with the exponential decay method.
%They additionally score the candidate articles using the category of the candidate articles and the preference of category in the session.
%
%Okura et al.~\cite{Yahoo2017} introduce their works to improve the news recommendation system serviced by Yahoo Japan.
%The main idea of their method (\compYahooName) is generating the article representation using the category information to resolve the Cold-Start problem.
%They prepare the triple of the article, the article with the same category, and with the other category.
%They proposed the model trained with the triple above to generate the article representations which are the closer between articles in the same category and farther in the other category.

\paragraph{\textbf{Evaluation Metrics}}
We evaluate the our methods and the competitors with the Hit Rate (HR)~\cite{EvalSessionRec} and the Mean Reciprocal Rank (MRR)~\cite{MRR}.
Given the rank of the selected article in the 20 candidate articles, the HR@5 and the MRR@20 score as follows:
\begin{align*}
	HR@5 &= \frac{1}{|S|}\sum_{c \in S}|\{r_c | r_c <= 5\}| \\
  MRR@20 &= \frac{1}{|S|}\sum_{c \in S} \frac{1}{r_c}
\end{align*}
where $S$ indicates the set containing all of the entries in the testset, $c \in S$ is an set of the candidate articles for the timestamp of prediction in the testset, and $r_c$ is the rank of the selected article by user.
The $|c|$ is the fixed to 20 as the output of the preprocessing.

\subsection{Preprocessing of the Data}
\label{sec:exp:preprocess}
We introduce the jobs done during the preprocessing phase.
The outputs of the which are shared with all of the methods to train and evaluate them.

% article representation
As mentioned before, we had crawled the sentences of the article from the URLs in the Adressa dataset.
To generate the article representation of them, we train the sentences with the Gensim~\cite{Gensim} to get the embedding vector of the article.
As the hyperparameters of the Doc2Vec~\cite{Doc2Vec}, we set the dimension of the embedding vector to 1000, the size of the window to 10, and initialize the alpha to 0.025 and descent by 0.001 at every 10 epochs.
In the case of the Glob dataset, we use the pretrained 250-dimensional vectors generated by the provider.

% RNN input
We need the session-based sequence with the selections of users for feeding the RNN-based model with.
One of the problem in the dataset is that most of the users are anonymous because they were not logged in when checking the news articles.
But the selection done in the same session is by the same user as long as the session remains alive.
To save the memory, we give the unique index integer for each article then represent the sequence of the selection of the article as the list of integers.
We generate the dictionary to keep the vector of the article representation related to the index integer to change the list of integers to the list of vectors when feeding the model with them.

% candidates
The common objective of the methods is to rank the given candidate articles to provide the recommendation for each user.
To do this, we need the list of candidates for all of the timestamp at the evaluation.
We process the following several steps to generate the candidate list.
First, we generate the timeline of article selection done by all users.
Second, we slide the window, the length of which is 3-hours, in the timeline.
Third, we count the number of selection for each article in the window.
The bigger number of selection means that the article is more popular. 
Last, we sort the articles by the descending order of popularity to collect the top-20 popular articles as the candidate list for the timestamp which is the latest timestamp in the window.

% popular and recent
The \lstmcat and the \method are needed to be fed with the trendy representation as the one of input domain.
As mentioned before, the attention layer generates a trendy representation with popular and recent articles at the specific timestamp.
To get the popular articles, we pick the top-5 popular articles in the candidates for the timestamp which is generated as above.
In the case of recent articles, we implement the similar window method as before and find the earliest timestamp of each article when first showing up in the window.
Then, we sort them in descending order of timestamp to get the top-3 articles recently showed up.

% category
Some of the competitors use the category information of articles to reinforce the article representation to alleviate the Cold-Start problem.
We prepare the one-hot encoded category vector for each article and provide the zero vector in case of the empty information.
There is 40-categories in the \dataAdressa and 460-categories in the \dataGloboName.


\subsection{Model Training}
\label{sec:exp:training}
We train all of the competitors and our methods using the same environment setting which contains the inputs, the loss function, the optimizer, the method for the early stop, and the hardware.
All of our experiments had been done on the workstation with the Intel Xeon E5-2630 CPU, the NVIDIA GTX 1080Ti GPU, and the 512GB RAM.

We sort the sequences in the dataset with the earliest timestamp for each sequence as the ascending order.
We divide the sorted sequences into the ratio 8:1:1 to get the training, validation, and test dataset, respectively.
By the sorting with timestamp, we show whether the model suffers from the Cold-Start problem because the new articles are published as the time passed on.
The longer period of the dataset means the more suffering from the Cold-Start problem.

When the traning phase, we set the batch size of the training data to 512 for feeding the model with.
The model generates the prediction vector of the next article for every step.
We calcuate the loss value using the prediction vector and the vector of actual next article with the Binary Cross Entropy (BCE) loss function which measures the difference between two vectors.
We use the Adam optimizer~\cite{Adam} to backpropagate the loss to update weights with the learning rate of 0.003.
We had tested the combinations with the other functions which are the Mean Squared Error (MSE) loss function, the Stochastic Gradient Descent (SGD) optimizer, and the Adagrad optimizer~\cite{AdaGrad}.
The combination of the BCE loss and the Adam is trained well.

We stop the traning if there is no improvement for the validation loss during the 10-epochs to prevent the overfitting which is so called early stopping~\cite{EalyStop}.
We limit the maximum number of epochs to 200.
All of the methods early stop the traning before reaching the maximum.

\begin{table}[ht]
	\renewcommand{\arraystretch}{1.2}
	\caption{
		Hyperparameters of the competitors and proposed methods.
		The parameters which is specialized in the specific method are shown on the Etc. column.
	}
	\label{tab:params}
\begin{tabular}{lcccc}
\toprule
	\textbf{Method} & \textbf{Hidden} & \textbf{Layer} & \multicolumn{2}{c}{\textbf{Etc.}} \\
\midrule
\compLSTMName & 1280 & 1 & \multicolumn{2}{c}{-} \\
\compGruRecName & 786 & 3 & \multicolumn{2}{l}{Dropout rate : 0.5} \\
\multirow{2}{*}{\compNaverName} & \multirow{2}{*}{1024} & \multirow{2}{*}{1} & \multicolumn{2}{l}{Decay rate : 0.1} \\
 & & & \multicolumn{2}{l}{Category : 0.7} \\
\multirow{2}{*}{\compYahooName} & \multirow{2}{*}{1024} & \multirow{2}{*}{2} & \multicolumn{2}{l}{Corruption : 0.001} \\
& & & \multicolumn{2}{l}{VAE hidden : 0.8} \\
\lstmcatname & 1440 & - & \multicolumn{2}{c}{-} \\
\methodname & 1440 & - & \multicolumn{2}{l}{Cell dropout rate : 0.3} \\
%\multirow{2}{*}{\methodname} & \multirow{2}{*}{1440} & \multirow{2}{*}{-} & \multicolumn{2}{c}{Second-Cell Dropout Rate} \\
%\cmidrule(lr){4-5}
% & & & \multicolumn{2}{c}{0.3} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Experimental Results}
\label{sec:exp:results}
\begin{table*}[t]
	\renewcommand{\arraystretch}{1.2}
	\caption{
		Evaluations of the proposed methods and the competitors by the metrics. We use the Hit Rate (HR@5) and the Mean Reciprocal Rank (MRR@20) to evaluate the methods.
		The higher value means better performance for both metrics and the best result in each row is marked in bold.
		The HR@5 focuses on the precision of the recommendation system and the MRR@20 evaluates the overall performance of the system.
		We mark our proposed methods in the $\dagger$ which is fed with input domains from article and trendy representation both.
	}
	\label{tab:evaluation}
	\begin{tabular}{llC{1.4cm}C{1.4cm}C{1.4cm}C{1.3cm}C{1.3cm}C{1.6cm}c}
\toprule
		\textbf{Dataset} & \textbf{Metric} & \textbf{\compPopName} & \textbf{\compLSTMName} & \textbf{\compGruRecName} & \textbf{\compNaverName} & \textbf{\compYahooName} & $\textbf{\lstmcatname}^\dagger$ & $\textbf{\methodname}^\dagger$ \\
\midrule
\multirow{2}{*}{\dataAdressaOneWeekName} \
	& HR@5   & 0.4988 & 0.4680 & 0.4335 & 0.4714 & 0.4569 & 0.8703 & \textbf{0.8948} \\
	& MRR@20 & 0.3291 & 0.3289 & 0.3196 & 0.3361 & 0.3341 & 0.6505 & \textbf{0.6728} \\
\cmidrule(lr){2-9}
\multirow{2}{*}{\dataAdressaTenWeekName} \
	& HR@5   & 0.5672 & 0.3690 & 0.3221 & 0.3677 & 0.3477 & 0.8805 & \textbf{0.9095} \\
	& MRR@20 & 0.3735 & 0.2466 & 0.2153 & 0.2461 & 0.2320 & 0.5986 & \textbf{0.6161} \\
\cmidrule(lr){2-9}
\multirow{2}{*}{\dataGloboName} \
	& HR@5   & 0.2845 & 0.3549 & 0.3291 & 0.3551 & 0.3537 & 0.6585 & \textbf{0.6779} \\
	& MRR@20 & 0.2001 & 0.2502 & 0.2248 & 0.2483 & 0.2500 & 0.4785 & \textbf{0.4864} \\
\bottomrule
\end{tabular}
\end{table*}

We show the evaluation of the competitors and our methods in table~\ref{tab:evaluation}.
We calculate the evaluation metrics from the second prediction for each sequence to give a chance to the model to find the features of the sequences.
The competitors, except \compPopName, only concern the personal preferences so their first prediction is weak comparing to the \compPopName, and our methods.
We exclude the first prediction to make a fair competition.
We do the grid-search to find the optimal hyperparameters which are shown in table~\ref{tab:params} where each of column represents that
the Hidden is for the dimension of hidden state,
the Layer means the number of layer of cell in the recurrent function,
the Dropout Rate is the rate of drop in the linear layer to generate prediction,
the Decay Rate and the Category are the degree of effect by the category information in the \compNaverName,
the Corruption and the VAE (Variational Auto Encoder) Hidden are for the VAE parameter to generate new article representation in the \compYahooName, 
and the Second-Cell Dropout Rate is the rate of the cell state for second input domain in the \methodname.
We train the model with the aforementioned way then calculate evaluation metrics using the 5000-samples in the test set to find the best hyperparameters.

The preferences of users in the \dataAdressa are more biased to the popular articles than the \dataGloboName.
The HR@5 of the \compPop for the \dataAdressa is more than 0.48.
This means that about half of selections had been done to the top-5 popular articles.
The HR@5 of the \compPop for the \dataGlobo is 0.1661 which is much lesser than the \dataAdressaName.
So the users of the \dataGlobo is less biased to the popular articles comparing to the \dataAdressaName.

Our methods overwhelm the competitors in all of the cases as shown in table~\ref{tab:evaluation}.
Our methods are more strong in the \dataAdressa which has the biased preferences of users to the popular articles.
The competitors, except \compPopName, do not consider the prevalent preferences so do not capture the features from the popular information at the specific timestamp.

\begin{figure}[ht]
\begin{center}
\begin{tabular}{c}
     \includegraphics[width=0.5\textwidth]{FIG/session.png} \\
\end{tabular}
\caption{
	The MRR@20 evaluation of different lengths of the session.
	Bigger session increases the accuracy of competitors and \methodname.
	\method keeps high accuracy even when the length of the session is short.
}
\label{fig:largerDataset}
\end{center}
\end{figure}

All of the methods more suffer from the Cold-Start problem in the \dataAdressaTenWeekName.
Because the interval of the timestamp between the training and the test dataset is larger in the \dataAdressaTenWeek than the \dataAdressaOneWeekName.
The Cold-Start problem decreases the performance of the methods, except \compPop which is not learning in the training phase, but our methods keep better performance comparing to the \compPop as shown in table~\ref{tab:evaluation}.

\begin{figure*}[t]
\begin{center}
\begin{tabular}{c}
     \includegraphics[width=0.9\textwidth]{FIG/candidates.png} \\
\end{tabular}
\caption{
	The MRR@20 evaluation on the different number of candidate articles.
	The accuracy of methods, except \compPopName, decreases with the larger number of candidates.
	\method keeps the best accuracy among the competitors.
}
\label{fig:candidates}
\end{center}
\end{figure*}

Our methods and the competitors rank the candidates which include actual next article chosen by the user.
The larger number of candidates means the harder problem to solve.
Our methods keep the better performance than the \compPop with a large number of candidates.
But the performance of the competitors, except \compPopName, decreases extremely as shown in figure~\ref{fig:candidates}.

\subsection{Discussion about \method}
\label{sec:exp:discuss}
Do the recent articles help the \method to increases the performance of the recommendation?
Our proposed method, the \methodname, aggregate the prevalent and the personal preferences to increase the accuracy of the recommendation.
We place the attention layer to generate the vector of the prevalent preferences from the popular and recent articles at the specific timestamp.
The importance of the popular articles is shown with the evaluation of the \compPop which ranks the candidates as the popularity of them and gives a strong performance of the recommendation.

\begin{table}[]
	\renewcommand{\arraystretch}{1.2}
	\caption{
		Weights for the recent articles in the attention layer.
		Average is for the avarage of all of cases in the testset.
		MedToHigh means that the \method gives high rank to the medium popular article and that is accurate.
		LowToHigh means that the \method gives high rank to the unpopular article and that is accurate.
	}
	\label{tab:attention}
\begin{tabular}{lC{1.6cm}C{1.6cm}C{1.6cm}}
\toprule
\textbf{Dataset} & \textbf{Average} & \textbf{MedToHigh} & \textbf{LowToHigh} \\
\midrule
\dataAdressaOneWeekName & 0.3960 & 0.3823 & 0.4389 \\
\dataAdressaTenWeekName & 0.3721 & 0.3732 & 0.4451 \\
\dataGloboName & 0.3821 & 0.3681 & 0.4732 \\
\bottomrule
\end{tabular}
\end{table}

We observe the weights of the attention layer to check whether the \method pays attention to the recent articles when predicting the next article.
Our observation is shown in table~\ref{tab:attention}.
The sum of weights for the recent articles would be 0.375 if the \method thinks that the importance of the articles is the same.
Note that we feed the attention layer with 5-popular and 3-recent articles.

We find the cases where the weights for the recent articles is much bigger than the average when the \method giving an accurate prediction.
We choose the cases where the \method gives top-5 rank to the next selected article which means the prediction of the \method is accurate.
We classify the cases by the rank of popularity for the next selected article in the candidate articles.
For example, if the rank of popularity is less or equal to 5, the article is classified into the popular group.
If the rank is more or equal to 15, the article is classified into the unpopular group.
And medium popular for the others.
The attention weights for the recent articles is bigger than the average when the \method gives an accurate prediction about the next selected articles in the low popular group.
The opposite cases are shown when the \method gives an accurate prediction about the next selected articles in the medium popular group.
We conclude that the \method pays more attention to the recent information when thinking that users would select the unpopular articles in this case.
And the \method pays more attention to the popular information when it rearranges the popular articles with the medium.

\begin{table}[h]
	\renewcommand{\arraystretch}{1.2}
	\caption{
		We exclude the attention layer or the cell dropout in the \method to show the effect of them.
	}
	\label{tab:without}
\begin{tabular}{llC{1.25cm}C{1.25cm}C{1.4cm}}
\toprule
	\textbf{Dataset} & \textbf{Metric} & \textbf{\methodWOAttName} & \textbf{\methodWODropName} & $\textbf{\methodname}^\dagger$ \\
\midrule
\multirow{2}{*}{\dataAdressaOneWeekName} \
	& HR@5 & 0.8928 & 0.8850 & 0.8948 \\
	& MRR@20 & 0.6705 & 0.6485 & 0.6728 \\
\cmidrule(lr){2-5}
\multirow{2}{*}{\dataAdressaTenWeekName} \
	& HR@5 & 0.8831 & 0.8712 & 0.9095 \\
 	& MRR@20 & 0.6010 & 0.5894 & 0.6161 \\
\cmidrule(lr){2-5}
\multirow{2}{*}{\dataGloboName} \
	& HR@5 & 0.6758 & 0.6655 & 0.6779 \\
 	& MRR@20 & 0.4749 & 0.4786 & 0.4864 \\
\bottomrule
\end{tabular}
\end{table}

Do the attention layer and the second cell dropout help the \method to do accurate recommendation?
We exclude the attention layer or the second cell dropout method in the \method to show the effect on the performance.
The \methodWOAtt represents the \method without the attention layer where the prevalent representation is the average of the popular and the recent article vectors at the specific timestamp.
The \methodWODrop removes the second cell dropout procedure in the \methodname.
The performance of the excluded methods is lower than the \method as shown in table~\ref{tab:without}.
The attention layer increases the flexibility of the \method to generate the prevalent preferences dynamically between the popular and the recent articles.
The second cell dropout prevents the \method lean to the personal preferences during the training.
We interpret the effect of this as the special case of the group dropout~\cite{GroupDropout}.

%Table \ref{tab:table1} shows that fake accuracy results from the experiments.
%Figure \ref{fig:fake} is generated by the GNUPlot to test how it look like on the paper.
%\begin{table}[ht!]
%	\begin{center}
%		\caption{A fake results evaluated by the matrics. Values indicate accuracy measured using each matric and 99\% confidence intervals.}
%		\begin{tabular}{c|c|c|c} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
%			\textbf{Model} & \textbf{AUC} & \textbf{MRR} & \textbf{nDCG} \\
%			\hline \hline
%			RNN & $0.582 \pm 0.003$ & $0.582 \pm 0.003$ & $0.582 \pm 0.003$\\
%			LSTM & $0.582 \pm 0.003$ & $0.582 \pm 0.003$ & $0.582 \pm 0.003$\\
%			GRU & $\textbf{0.682} \pm \textbf{0.003}$ & $\textbf{0.682} \pm \textbf{0.003}$ & $\textbf{0.682} \pm \textbf{0.003}$\\
%		\end{tabular}
%		\label{tab:table1}
%	\end{center}
%\end{table}
%
%\begin{figure}[htb]
%\begin{center}
%\begin{tabular}{cc}
%     % uncomment the next lines, and give the right ps files
%     \includegraphics[width=0.5\textwidth]{FIG/fake_accuracy.png} \\
%     %\psfig{figure=FIG/plot.ps,width=2in} \\
%     % \psfig{figure=FIG/data.ps,width=2in} &
%     % \psfig{figure=FIG/plot.ps,width=2in} \\
%\end{tabular}
%\caption{A fake perfomance plot which demonstrates the number of successful prediction via the number of training set}
%\label{fig:fake}
%\end{center}
%\end{figure}

