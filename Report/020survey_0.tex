\subsection{Playing Atari with Deep Reinforcement Learning}
해당 논문은 딥러닝 알고리즘으로 action-value function의 Q 값을 예측하는 기법인 Deep Q-Networks (DQN)을 제시하였다. 
논문 제목에서도 볼 수 있듯이 새로운 메소드를 Atari 2600이라는 고전 게임 모음 중 대표적인 7 개의 게임을 통해 그 성능을 확인하였다. 
Atari 2600 게임은 action에 대한 transition 확률과 reward가 명시적으로 주어지지 않은 model-free 문제이다.

해당 논문에서 볼 수 있는 특이한 점은 입력 값으로 게임의 이미지 전체를 받는다는 점인데, 기존 강화 학습에서 state를 정의할 때 게임 이미지의 각 픽셀을 하나의 state로 정의하여 에이전트에게 전달하는 것과는 다르면서 색다른 방식이다. 
따라서 게임의 이미지 화면을 입력 값으로 받으면 딥러닝 알고리즘에서 이미지 처리에 탁월한 Convolutional Neural Networks (CNN)을 통해 계산된 Q값을 에이전트에게 전달하고 해당 값을 기반으로 최적의 action을 실행하여 게임을 진행하는 방식으로 구성되어 있다. 

여기서 눈 여겨 봐야할 점은 Q값을 구하기 위해 CNN을 사용하였다는 것인데, 이는 기본적으로 사용되는 value iteration 알고리즘과 같은 문제점을 해결한다. 
Value iteration 알고리즘은 논문과 같은 환경인 transition 확률과 reward가 명시적으로 주어지지 않은 조건에서는 동작하기 어려운 점이 있다. 
그러나 해당 논문에서 사용하는 Q-Networks는 이러한 model-free 문제에서 neural network value function approximator를 통한 샘플링을 통해 Q값을 구한다. 
따라서 학습을 할 때 Q-network에서는 손실 함수를 최소화하면서 그 성능을 높인다. 

논문에서 보여준 이러한 방식은 우리가 풀려는 슈퍼마리오 게임에서의 환경과 비슷한 점이 많기 때문에 논문에서 제시한 DQN 알고리즘을 사용할 예정이다. 
슈퍼마리오 게임 또한 Atari 게임과 같이 게임 화면이 state로 주어지고 finite한 게임 내에서 게임이 한 에피소드를 끝내기 위한 조건도 유사하기 때문에 논문에서 제시한 알고리즘을 사용한다면 높은 성능을 보일 것이라 판단한다. 

그러나 해당 논문에서 제시한 DQN은 Q-learning 값을 업데이트 하는 과정에 있어서 max 연산자를 사용하기 때문에 Q-value를 실제보다 높게 평가하고 그 결과 학습이 느려지는 경향이 있다. 
따라서 실험에 사용한 Atari 게임은 우리가 풀려는 슈퍼마리오 게임보다 상대적으로 크기가 작고 간단하기 때문에 슈퍼마리오 게임과 같이 더 크고 복잡한 조건에서도 과연 학습이 느려질 수 있다는 것에 대한 제약사항이 존재할 가능성이 있다. 

