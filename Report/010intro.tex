본 대규모 데이터분석 특강 프로젝트에서 우리는 슈퍼 마리오 브라더스 게임을 강화학습을 통해 컴퓨터가 학습하고 최종적으로 가능한 빠르게 스테이지를 클리어하는 것을 목표로 한다.

슈퍼 마리오 브라더스에서 조작가능한 키는 6가지이며, 키들을 조합하여 16가지의 유의미한 action을 입력할 수 있다.
%사용 가능한 동작은 총 6 가지로 구성되어 있으나 서로 다른 키를 조합하여 입력하는 것이 가능하므로 실제로 사용 가능한 동작은 총 16개로 구성되어 있다.
해당 게임은 agent인 마리오가 목적지에 도달하거나, 적에게 부딪히거나, 제한 시간 내에 목적지에 도달하지 못하면 stage가 끝나게 되는 episodic한 요소를 가지고 있다. 

본 프로젝트는 강화학습을 통해 슈퍼 마리오 브라더스를 플레이하는 모델을 구현하는 것이다.
또한 단순히 플레이 하는 것을 넘어서서 빠른 시간 내에 최종 목적지에 도달하는 것을 목표로 한다. 
실제 인간에게 주어지는 것처럼 현재 화면을 입력 값으로 받고, 이를 convolutional neural network (CNN)을 통해 해석한 후, 이를 기반으로 현재 state를 파악하여 앞으로의 reward를 극대화시키기 위한 동작(action)을 찾아 내도록 한다.
강화학습의 학습 효율과 그로부터 얻어낸 최적화된 policy의 가치를 극대화하기 위해서 강화학습 분야에 발표된 최신 기법들과 더불어 슈퍼 마리오 브라더스에 특화한 다양한 방법을 시도하였다.
최종적으로 발표된 기법 중 하나인 Deep Q-Network (DQN) 기법을 근간으로 확장한 Split Attention DQN 기법을 소개하고 실험한 것에 대한 결과에 대해 설명하며 그 성능을 측정한다. 
%따라서 해당 게임을 강화학습을 통해 클리어하기 위해 실제 인간에게 주어지는 것처럼 현재 화면을 입력 값으로 받고 이를 통해 최적의 동작(action)을 찾아내는 모델인 convolutional neural network (CNN)을 구축하는 것을 기반으로 다양한 시도를 할 것이다.

프로젝트 기간내에 효율적으로 시간을 분배해 최적의 결과를 도출하기 위해 각 구성원의 역할을 개발환경 세팅, 기존 방법 학습, 새로운 방법 연구, 제안 방법 개발과 같은 방식으로 세분화하여 해당 기법을 구상하였다.

해당 프로젝트 제안서의 구성은 다음과 같다. 
섹션~\ref{sec:survey}에서는 본 프로젝트에서 풀려고 하는 슈퍼 마리오 브라더스 게임을 학습하고 목표를 달성하기 위해 도움이 될만한 기존 논문의 메인 아이디어를 간략히 설명하고 어떻게 프로젝트 구현에 사용될 수 있을지에 대해 설명한다. 
또한 논문의 메소드를 본 프로젝트에서 사용함에 있어 제약사항 혹은 문제가 될 여지가 있다면 이러한 부분에 대해 서술한다. 
섹션~\ref{sec:preliminaries}에서는 슈퍼 마리오 브라더스 게임의 동작 방식과 게임을 이해하기 위한 요소들을 설명한다.
섹션~\ref{sec:method}에서는 강화학습을 진행하는데 필요한 state, action, 그리고 reward이 게임에서 어떻게 구성되어 있는지에 대해 설명한다. 
또한 기존 발표된 기법인 Deep Q-Network (DQN)을 기반으로 구현한 Split Attention DQN의 구현 방식과 메커니즘에 대해 설명한다.
섹션~\ref{sec:plan}에서는 본 프로젝트에서 사용할 라이브러리에 대한 설명과 각 구성원의 개발 일정에 따른 역할에 대해 설명한다. 
섹션~\ref{sec:experiments}에서는 섹션~\ref{sec:method}에서 선보인 Split Attention DQN의 성능에 대한 실험 결과에 대해 서술한다.
마지막으로 섹션~\ref{sec:conclusion}에서는 해당 제안서에서 설명한 부분들에 대해 정리한 뒤 본 프로젝트 제안서를 마무리하는 것으로 구성되어 있다.
