%\documentclass[sigconf, review, anonymous, screen]{acmart}
\documentclass[10pt]{article}
\usepackage[top=3cm, bottom=3cm, left=3cm, right=3cm]{geometry}
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{kotex}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\usepackage[ruled,vlined,linesnumbered]{algorithm2e}

\usepackage{booktabs} % For formal tables
\usepackage{subcaption} % for subfigures

\usepackage{fancyhdr}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{mdwlist}
\usepackage{indentfirst}
\usepackage{physics}
\usepackage{xspace}
\usepackage{tablefootnote}
\usepackage[bottom]{footmisc}
\usepackage{footnote}
\usepackage{hyperref}
\usepackage{makecell}
\usepackage{subcaption}
\usepackage[bottom]{footmisc}
\usepackage{balance}
\xspaceaddexceptions{]\}}

\renewcommand{\baselinestretch}{1.3}

% Definitions
\input{./dfn}
%\setlength{\belowcaptionskip}{-0.3cm}
\begin{document}
\title{[Super Mario RL] 대규모 데이터분석 특강 - 프로젝트 보고서}

\author{
구본헌 \& 김정훈\\
Department of Computer Science and Engineering\\
Seoul National University\\
\texttt{\{darkgs, rlawjdgns434\}@snu.ac.kr}\\
\texttt{2019-\{36050,29394\}}\\
}

\renewcommand{\baselinestretch}{1.25}

\maketitle
% Sec 0. Abstract
\begin{abstract}
본 프로젝트 보고서는 2019년도 봄학기 대규모 데이터분석 특강 과목의 조별 프로젝트의 내용을 담고 있다.
우리는 슈퍼 마리오 브라더스 게임을 강화학습을 통해 높은 점수로 플레이하는 모델을 개발하는 것을 목표로 한다.
이를 위해 우리는 먼저 게임 플레이를 대상으로 하는 강화학습 기법들에 대해 발표된 논문을 조사하여, 우리 모델을 구성할 때 고려할 점과 아이디어를 얻기위한 기반으로 삼았다.
또한, 슈퍼 마리오 브라더스 게임을 분석하여 강화학습 모델에 어떻게 적용할지 알아보고 구체적인 개발환경을 조사한 후 일정을 수립하였다.
\end{abstract}

%\keywords{
%Reinforcement Learning; 강화 학습
%}


% Sec 1. Introduction
\section{Introduction}
\label{sec:intro}
\input{./010intro}

\section{Paper Survey}
본 절에서 우리는 게임에 적용된 강화학습 기법을 소개한 논문을 조사한 결과를 소개한다.
먼저 아타리 게임을 통한 강화학습 연구의 시초인 Deep Q-Networks (DQN)을 하위 섹션~\ref{sec:survey:DQN}에서 다루며, 이를 발전시킨 Dueling Network를 다룬 논문을 하위 섹션~\ref{sec:survey:DuelNet}에서 소개한다.
Policy gradient 방법으로 넘어가서, deterministic policy gradient (DPG)를 하위 섹션~\ref{sec:survey:DPG}에서 소개한 후,
이에 actor-critic 개념을 입혀 개량한 deep deterministic policy gradient (DDPG)를 하위 섹션~\ref{sec:survey:DDPG}에서 다룬다.
GPU 가속을 통한 병렬처리 컴퓨팅 환경을 효율적으로 이용하도록 설계된 Asynchronous Actor-Critic (A3c)의 컨셉에 대해 하위 섹션~\ref{sec:survey:A3C}에서 알아본다.
마지막으로 action의 선택과 평가에 쓰이는 weight set을 분리하여 value값이 과평가되는 것을 방지하는 Double Q-learning에 대해 하위 섹션~\ref{sec:survey:DoubleQ}에서 소개한다.
\label{sec:survey}
\input{./020survey_0}
\input{./020survey_1}
\input{./020survey_2}
\input{./020survey_3}
\input{./020survey_4}
\input{./020survey_5}

\section{Preliminaries}
\label{sec:preliminaries}
\input{./030preliminaries}

\section{Proposed Method}
\label{sec:method}
\input{./040proposed}

\section{Development Settings and Plan}
\label{sec:plan}
\input{./050plan}

\section{Experiments}
\label{sec:experiments}
\input{./060experiments}

\section{Conclusion}
\label{sec:conclusion}
\input{./070conclusion}

%\newpage
%\balance
\bibliographystyle{plain}
\bibliography{paper}

\end{document}

